# Speech

Good afternoon, everyone

my name is xxx and my supervisor is xxx

The topic I will give today is xxx



---

### 2

I will first introduce the background of generative adversarial networks

and then follow my research problem of conditional Generation

Before my own work, I will give an overview of related work

The last part is future work and my conclusion

---

### 3 

To begin with, Let’s look at two images,

### 4

one is a real face, and the other is a fake face generated by GAN

Which face do you think is real?

---

### 5

Oh, actually the right face is real. 

This is the state of the art example of GAN

---

### 6

And here, Prof. Yann had said that xxx

An art painting generated by GAN sold for $400 thousand at Christie's

the technology of GAN is also used in Adobe photoshop



After we have established a perceptual understanding of GAN, we should start to talk about some details

---

### 7

Let's begin with the explanation of GAN. You may have always heard of this term.

G is from Generative, which means the purpose or the result is getting new generative data rather than predictions of labels.

A is from Adversarial, this framework is composed by two parts: a generator and a discriminator. generator want to generate fake data to fool discriminator, but the discriminator doesn't want to be fooled. So it's an adversarial process.

N is from Neural Networks, which means the generator and discriminator are represented by neural networks.

---

### 8

A framework of GAN is made by Silva

we can see that the generator turns a random noise into a fake image

and then the discriminator will distinguish the fake image and the training set

Their respective losses will help them get better



from my point of view, the discriminator learns to map features to labels, while the generator learns to map labels to featurs

---

### 9

The details of GAN can be also represented by this diagram

I will use this unified form in my whole report.



we first Sample...



We can use these two maximization functions to update the discriminator and generator,  the Intuitive understanding is that the role of discriminator is scoring. Here for discriminator, We want to give high marks to the real samples and low marks to the fake samples. And for generator, we want to give high marks to the fake samples.

---

### 10

After the background, let’s talk about my research problem

### 11

I want to make GAN controllable

we get a projection from latent space z (which is input) to observe space x (which is output).

One of the problem of GAN is that we can not control the output,

However, in the real world, for example, the real data has two categories, what we want to achieve is that given a certain class of z, we can get a certain class of x

---

### 12

How to make it, actually the latent space are entangled with each other, maybe several dims of latent space control a same feature dim. if we want to make GAN controllable, we need to disentangle them. After that, we can control one feature by modifying one dim of latent space.

---

### 13

There are some previous work related to me

### 14

So here comes the details of Conditional GAN, 

The first two steps are the same, but here we need a pair of real samples with true labels

and we need extra real samples with false labels .



The discriminator now has to score three parts, give high marks to the real samples with true labels, low marks to the fake samples with true labels, and low marks to the real samples with false labels



the function of the generator is same.

---

### 15

To introduce a new framework, here is a supplementary point about semi-supervised learning

On the left, Traditional supervised learning is data with labels, the two cycles are labeled by two colors. we can easily train a classifier to classify these two parts. 

On the right, here we only have few labeled data, we can first use the unlabeled data to cluster them into two parts, and Then use a limited labeled data to do the classifier

We can use GAN to generate more unlabeled data to achieve semi-supervised learning

---

### 16

This is the detail of Semi-supervised GAN, 

Discriminator of Conditional GAN is still a binary classification work. We are wondering whether Discriminator can classify the label of data while distinguishing real and fake

The details here are similar

The difference in update function is that we use a multi-class cross entropy to replace binary cross entropy.



However, SEMI-SUPERVISED GAN does not achieve the effect of Conditional GAN, but only makes Generator performs better.

---

### 17

Now can we get the advantage of Semi-supervised GAN and achieve controllable GAN at the same time?

Traditional GANs extract features that distinguish between real and fake, while Conditional GANs extract features that distinguish between paired data. Now, one task is to extract features of real and fake, while the other task is to extract features on label classification.

These two tasks can be done at the same time, and both work will be better.
Finally we will get a balance between these three.



So we use two networks to achieve them separately

C means a classifier, and write it with D because they share some same weights



The update function of discriminator is same

for the classifier, we want to give high marks to real samples with true labels, low marks to real samples with false labels, and high marks to fake samples with true labels.

The update function of generator is that we want to give high mark to both fake samples and fake samples with true labels.

---

### 18

After introduce the related work, Let’s talk about my own work.

### 19

My motivation is that a label or a trained model, which one is easy to get, 

 

We have known that there are many trained models and some new data without labels. How to utilize the capacity trained by others is my consideration 

## 20

Then can we generate American female face with only the data of American male face 

can we use a gender classifier trained by Chinese male face and female face. because the feature of gender is similar or same.

### 21

And can we use a moon classifier to add moon into a landscape image?

### 回头

These two situation can not be solved by previous work

because they can only capture existing features

the trained model I use has already obtain the ability to extract features that do not exist

---

### 22

With this in mind, I propose a new framework of Multi-objective GAN

we use a trained model, this trained model has learned to extract a specific feature, So it can add new features to original data which do not have.



For an example, if the subject of data are all in the center, then the position is not a feature, but a trained position model can help the generator produce this specific feature.



and the function is simple, we only need to add a classifier loss to generator. The optimization problem of these two objectives will come up later

---

### 19

This is a comparison between Multi-objective GAN and related GANs.

You can see the main difference is that we no longer need labels of data.



Next, we need to know how to implement

---

### 24

A multi-objective optimization problem can be formulated as this function,

Because GAN use neural networks, we choose a gradient-based method to solve,

And we use a weighted-sum approach to decompose primal problem.



So the problem comes to how we choose the weight.

---

### 25

We use an algorithm of Multi-gradient descent to find a common descent direction 



And To solve easily,  the definition can be reformulated as a Quadratic Constrained Optimization Problem

An intuitive understanding is that the direction of descent is perpendicular to this line

---

### 26

In addition to the exact calculation, we can also make a rough estimate

this is because the direction you compute is not necessarily the best

We can use these function to calculate the part that feasible descent direction can be

---

### 27

A solution is pareto stationary, when the common direction satisfied this condition

And we use Gradient Normalization as a trick to constrain the scales

This figure shows the optimization process

---

### 28

This is the experiment result,

It shows that we achieve the expectation and the controllability of GAN 

---

### 29

In the future work

### 30

I will focus on representation learning with GAN, learn a factorized representation , which means changes in one factor only affect the corresponding elements in the generated data, while invariant to other factors



The left figure show the control of the angle of cars, and the right figure show the control of the color of cars 



And I will apply this technique on 3D neural rendering

---

### 31

I have mentioned Entangle & Disentangle of GAN earlier

But now we hope to disentangle more thoroughly

---

### 32

Let’s conclude whole work, 

1. we mainly propose a Muti Objective GAN framework. We extend GAN with some extra objectives to help control the generated process. 
2. The novelty of our work is that we can generate new features which are not existing before. And we can achieve trade off between sample diversity and quality
3. The methods we use including a multi gradient descent algorithm and evolutionary paradigm
4. The result shows that we have make similar or better performance with previous work. And more convincing experiments will be shown in the future



Thank you so much for listening!